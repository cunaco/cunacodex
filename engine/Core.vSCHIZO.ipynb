{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPbVzqz/+GK8PCiBNPL25eR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"id":"SFje4greZpgc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714310678542,"user_tz":-180,"elapsed":243258,"user":{"displayName":"Daniel Ilkiv","userId":"16566585594080919491"}},"outputId":"f96d8a99-4ca6-4bf6-8730-8ddd959ddaa0"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Collecting neo4j\n","  Using cached neo4j-5.20.0.tar.gz (202 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j) (2023.4)\n","Building wheels for collected packages: neo4j\n","  Building wheel for neo4j (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for neo4j: filename=neo4j-5.20.0-py3-none-any.whl size=280771 sha256=4ec62a6496376d07885d0afe707fcc38e2083b130e6e302c038d1ddd87643b6a\n","  Stored in directory: /root/.cache/pip/wheels/cb/12/66/764554d079caad4b9a11a02cfc0d200dd876d12935b9cf7e64\n","Successfully built neo4j\n","Installing collected packages: neo4j\n","Successfully installed neo4j-5.20.0\n"," * Serving Flask app '__main__'\n"," * Debug mode: on\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n"," * Running on http://127.0.0.1:5000\n","INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n","INFO:werkzeug: * Restarting with stat\n"]}],"source":["!pip install neo4j\n","\n","import random\n","import numpy as np\n","import spacy\n","import flask\n","import neo4j\n","import tkinter\n","import tensorflow\n","import keras\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","from flask import Flask, request, jsonify\n","from neo4j import GraphDatabase\n","\n","# Connect to Neo4j database\n","uri = \"neo4j+s://1b2ac73b.databases.neo4j.io\"\n","username = \"neo4j\"\n","password = \"tiJqZcftZSqmzbBeD5QqCBbDVUhlKEeBEslYRW5ok0Y\"\n","driver = GraphDatabase.driver(uri, auth=(username, password))\n","app = Flask(__name__)\n","\n","# Define API routes\n","@app.route('/query', methods=['POST'])\n","def query():\n","    data = request.json\n","\n","# Создание дампа из библиотеки\n","data = {\n","    CELEST1A = {\n","    'MIND': ['CAPACITY', 'COMPUTING', 'DIRECTION', 'REFLECTION'],\n","    'BODY': ['STRUCTURE', 'APPEARENCE', 'COMPLEXITY', 'STRENGTH'],\n","    'SOUL': ['IMAGE', 'MORAL', 'PASSION','WEIGHT'],\n","    'WILL': ['CUT','GO','PICK','SPACE']\n","}\n","\n","    C2GB = {\n","    'RELIGION': ['PERCEPTION', 'CONCEPTION', 'ACCEPTION', 'RECEPTION'],\n","    'PHILOSOPHY': ['RATING', 'TOPICAL', 'TERMINAL', 'COMPLEX'],\n","    'ART': ['PAST', 'PRESENT', 'FUTURE', 'ETERNAL'],\n","    'SCIENCE': ['CROWN', 'FRUIT', 'TRUNK', 'ROOTS']\n","}\n","    N4M7C = {\n","     'C1': ['cancellation'],\n","     'C2': ['control'],\n","     'C3': ['cinematics'],\n","     'C4': ['configurative'],\n","     'C5': ['care'],\n","     'С6': ['capable'],\n","     'С7': ['cheer']\n","}\n","    R4ST = {\n","    'FEED': ['INFO','FOOD','MANA','GRACE'],\n","    'STIMULUS': ['COMMUNICATION','MASTER','ARCADE','FREEPLAY'],\n","    'PULSE': ['HIGH','MIDDLE','LOW','OUT'],\n","    'SYNC': ['EXPERIENCE','PRODUCT','RELATION','SOLVERY']\n","}\n","}\n","\n","\n","    with driver.session() as session:\n","        result = session.run(query)\n","        response = [dict(record) for record in result]\n","\n","    return jsonify(response)\n","\n","if __name__ == '__main__':\n","    app.run(debug=True)\n"]},{"cell_type":"code","source":["from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","\n","# Load GPT-2 model and tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","\n","def generate_response(prompt):\n","    inputs = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n","    response_ids = model.generate(inputs, max_length=100, temperature=0.7, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n","    response = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n","    return response\n","\n","if __name__ == '__main__':\n","    while True:\n","        prompt = input(\"User: \")\n","        response = generate_response(prompt)\n","        print(\"Bot:\", response)\n"],"metadata":{"id":"ZXdRp7D5klTK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f4563f8a-aec0-401c-e6d3-48bb21e87098"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["User: Tell me about some of your data classification\n","Bot: Tell me about some of your data classification practices.\n","\n","I'm not sure what you're talking about. I'm not sure what you're talking about. I'm not sure what you're talking about.\n","\n","I'm not sure what you're talking about. I'm not sure what you're talking about.\n","\n","I'm not sure what you're talking about. I'm not sure what you're talking about.\n","\n","I'm not sure what you're talking about. I'm\n"]}]}]}